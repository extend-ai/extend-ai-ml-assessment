{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to complete the task in two ways:\n",
    "1.\tAt first, I want to try a classification approach to tackle this problem.\n",
    "2.\tI want to develop a segmentation approach to accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "We can divide the classification into two sections:\n",
    "1.\tData preparation\n",
    "2.\tTrain and validate my custom made model\n",
    "3.\tTest the dataset given by Extend AI\n",
    "## Libraries selection\n",
    "I can use PyTorch as I have good expertise in this deep learning library. However, to train the dataset, I used RAM-Net architecture which is built on Tensorflow and Keras. Therefore, I choose TensorFlow and Keras to tackle this problem.\n",
    "Model Description\n",
    "I developed this Residual Attention Mobilenet (RAM-Net) in the last year for medical image analysis, and this is a published work at ICMLA 2020 conference. This is a mobile-friendly computer vision network with fewer parameters and flops with the capabilities to achieve higher performance on image datasets. So, I decide to choose this model as I am confident enough about its performance.\n",
    "## Dataset Preparation\n",
    "I use the offline data augmentation technique to produce a total of 357 images (51*7 where 51 images from each image) from these real 7 images (which are provided by extend AI). I split this dataset by training and validation. Here I use 90% data for training and 10% data for validation. Furthermore, I use these real 7 images for testing.\n",
    "Here, I try to perform a binary classification problem where I divide the dataset into two classes: non_problametic (those images which do not have any issues) and problematic (those images which have knots and spots)\n",
    "In the test dataset, two images are regarded as non_problematic images, and five images are counted as problematic images. Therefore, in the augmented dataset, I have 102 images for the non_problematic class and the remaining 255 images for the problematic class.\n",
    "Data Augmentation Functionality: rotation, width shift, height shift, shear, zoom, horizontal flip, and fill mode=nearest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import os\n",
    "from glob import glob\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from sklearn.metrics import classification_report\n",
    "import functools\n",
    "import keras\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import Model\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications import MobileNet\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping,ModelCheckpoint\n",
    "import random\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D,GlobalMaxPooling2D,DepthwiseConv2D,Concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from at import augmented_conv2d\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize the whole dataset to 224X224, then normalize the dataset, and at last convert it to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:  (321, 224, 224, 3) (321,)\n",
      "validation set:  (36, 224, 224, 3) (36,)\n",
      "(321, 2)\n",
      "(36, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train = np.load(\"D:/polynomial/covid19/anomaly dataset/new_224_224_train.npy\")\n",
    "y_train = np.load(\"D:/polynomial/covid19/anomaly dataset/new_train_labels.npy\")\n",
    "X_val = np.load(\"D:/polynomial/covid19/anomaly dataset/new_224_224_val.npy\")\n",
    "\n",
    "y_val = np.load(\"D:/polynomial/covid19/anomaly dataset/new_val_labels.npy\")\n",
    "\n",
    "print('train set: ', X_train.shape, y_train.shape )\n",
    "print('validation set: ',X_val.shape, y_val.shape)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "#print(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Pre-trained weight from ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function augmented_conv2d at 0x00000251FFB17EA0> <class 'function'>\n"
     ]
    }
   ],
   "source": [
    "f=augmented_conv2d #attention augmented convolution\n",
    "print(f,type(f))\n",
    "\n",
    "#take pretrained weight of imagenet for thr last few classifier layers\n",
    "pre_trained_model = MobileNet(input_shape=(224,224, 3), include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pre_trained_model.layers:\n",
    "    #print(layer.name)\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(pre_trained_model.layers))\n",
    "last_layer = pre_trained_model.get_layer('conv_pw_13_relu')\n",
    "#print('last layer output shape:', last_layer.output_shape)\n",
    "last_output = last_layer.output\n",
    "x=GlobalAveragePooling2D()(last_output)\n",
    "#x=Dense(512,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "#x = Dropout(0.5)(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "# Add a dropout rate of 0.7\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "# Add a dropout rate of 0.7\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "#x = layers.Dense(128, activation='relu')(x)\n",
    "# Add a dropout rate of 0.7\n",
    "#x = layers.Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "# Add a final sigmoid layer for classification\n",
    "x = layers.Dense(2, activation='softmax',name='visualized_layer')(x)\n",
    "\n",
    "model=Model(pre_trained_model.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Online Data Augmentations\n",
    "\n",
    "Loss function: Binary cross-entropy \n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "In addition, I also use online data augmentation for the training.\n",
    "\n",
    "Online data augmentation function: rotation, width shift, height shift, shear, zoom, and fill mode=nearest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "visualized_layer (Dense)     (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 3,885,506\n",
      "Trainable params: 656,642\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 5s 498ms/step - loss: 0.9783 - acc: 0.5844 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5643 - val_acc: 0.7812 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 3s 331ms/step - loss: 0.8359 - acc: 0.6626 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.8075 - val_acc: 0.2500 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 4s 353ms/step - loss: 0.5797 - acc: 0.7570 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.6817 - val_acc: 0.7188 - val_top_k_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Configure and compile the model\n",
    "'''\n",
    "smooth = 1.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "'''\n",
    "#top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "\n",
    "#top3_acc.__name__ = 'top3_acc'\n",
    "\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy','top_k_categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=60, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                   shear_range=0.2, zoom_range=0.2, fill_mode='nearest')\n",
    "\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_datagen.fit(X_val)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "history = model.fit_generator(train_datagen.flow(X_train,y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = val_datagen.flow(X_val, y_val),\n",
    "                              verbose = 1, steps_per_epoch=(X_train.shape[0] // batch_size),\n",
    "                              validation_steps=(X_val.shape[0] // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "\n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # defining name basis\n",
    "    #conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    #bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value. You'll need this later to add back to the main path.\n",
    "    X_shortcut = X\n",
    "    #print(X_shortcut)\n",
    "\n",
    "    # First component of main path\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    #extra\n",
    "\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def convolutional_block(X, f, filters, s=2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "\n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # defining name basis\n",
    "    #conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    #bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path\n",
    "    X = Conv2D(F1, (1, 1), strides=(s, s),  kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "\n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid',\n",
    "                        kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3)(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "'''for layer in model.layers[:20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model.layers[20:]:\n",
    "    layer.trainable = True'''\n",
    "for layer in model.layers[:12]:\n",
    "    layer.trainable=True\n",
    "for layer in model.layers[12:13]:\n",
    "    layer.trainable=DepthwiseConv2D(64,strides=(2, 2),dilation_rate=(1,1))\n",
    "for layer in model.layers[13:25]:\n",
    "    layer.trainable=True\n",
    "for layer in model.layers[25:26]:\n",
    "    layer.trainable=DepthwiseConv2D(128,strides=(2, 2),dilation_rate=(1,1))\n",
    "for layer in model.layers[26:38]:\n",
    "    layer.trainable=True\n",
    "for layer in model.layers[38:39]:\n",
    "    layer.trainable=DepthwiseConv2D(256,strides=(2, 2),dilation_rate=(2,2))\n",
    "\n",
    "for layer in model.layers[39:75]:\n",
    "    layer.trainable=True\n",
    "    #layer.trainable=identity_block(layer.output,3,[64, 64, 256], stage=1, block='a')\n",
    "for layer in model.layers[75:76]:\n",
    "    layer.trainable=DepthwiseConv2D(512,strides=(2, 2),dilation_rate=(2,2))\n",
    "'''\n",
    "for layer in model.layers[76:81]:\n",
    "    layer.trainable=identity_block(layer.output,3,[128, 128, 512], stage=2, block='a')\n",
    "    #layer.trainable=True'''\n",
    "for layer in model.layers[76:78]:\n",
    "    ca1=convolutional_block(layer.output, f = 3, filters = [64, 64, 256],s=1)\n",
    "    a1=identity_block(ca1, 3, [64, 64, 256])\n",
    "    b1=identity_block(a1,3,[64, 64, 256])\n",
    "    #ba1=BatchNormalization()(b1)\n",
    "    ac1= augmented_conv2d(b1, 512)\n",
    "    ca2 = convolutional_block(ac1, f=3, filters=[128, 128, 512], s=1)\n",
    "    a2 = identity_block(ca2, 3, [128, 128, 512])\n",
    "    b2 = identity_block(a2, 3, [128, 128, 512])\n",
    "    c2 = identity_block(b2, 3, [128, 128, 512])\n",
    "    #ba2=BatchNormalization()(c2)\n",
    "    ac2=augmented_conv2d(c2,512)\n",
    "    ca3= convolutional_block(ac2, f=3, filters=[256, 256, 1024], s=1)\n",
    "    a3 = identity_block(ca3, 3, [1256, 256, 1024])\n",
    "    b3 = identity_block(a3, 3, [256, 256, 1024])\n",
    "    c3 = identity_block(b3, 3, [256, 256, 1024])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #layer.trainable=Concatenate([ca,a,b])\n",
    "\n",
    "    layer.trainable=c3\n",
    "    #print('man')\n",
    "    layer.trainable=True\n",
    "for layer in model.layers[78:81]:\n",
    "    layer.trainable=True\n",
    "for layer in model.layers[81:82]:\n",
    "\n",
    "    #res2=identity_block(res1,3,[128, 128, 512], stage=3, block='b')\n",
    "    #res3=identity_block(res2,3,[128, 128, 512], stage=3, block='c')\n",
    "\n",
    "    a=DepthwiseConv2D(1024, strides=(2, 2),dilation_rate=(4,4))\n",
    "    b=DepthwiseConv2D(1024, strides=(2, 2),dilation_rate=(8,8))\n",
    "    c=DepthwiseConv2D(1024, strides=(2, 2),dilation_rate=(16,16))\n",
    "    d=Concatenate([a,b])\n",
    "    layer.trainable=Concatenate([d,c])\n",
    "for layer in model.layers[82:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "\n",
    "epoch= 40 \n",
    "batch size=32\n",
    "learning rate=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "visualized_layer (Dense)     (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 3,885,506\n",
      "Trainable params: 3,863,618\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 8s 772ms/step - loss: 0.2429 - acc: 0.8906 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0200 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to D:/polynomial/covid19/anomaly dataset/RAM_Net.h5\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.3146 - acc: 0.8520 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0533 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 4s 357ms/step - loss: 0.1054 - acc: 0.9653 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0123 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 1.00000\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 4s 355ms/step - loss: 0.0772 - acc: 0.9811 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 1.00000\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 4s 366ms/step - loss: 0.0878 - acc: 0.9684 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 1.00000\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 4s 366ms/step - loss: 0.1706 - acc: 0.9056 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.8714e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 1.00000\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 4s 380ms/step - loss: 0.0538 - acc: 0.9842 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.5762e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 1.00000\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 4s 381ms/step - loss: 0.0774 - acc: 0.9874 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7320e-05 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 1.00000\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 4s 397ms/step - loss: 0.0282 - acc: 0.9905 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.5864e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 1.00000\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 4s 410ms/step - loss: 0.0418 - acc: 0.9842 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.4340e-05 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 1.00000\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 4s 421ms/step - loss: 0.0407 - acc: 0.9905 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0831e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999259090306e-06.\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 1.00000\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 5s 477ms/step - loss: 0.0198 - acc: 0.9969 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.1782e-05 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 1.00000\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 5s 462ms/step - loss: 0.0454 - acc: 0.9842 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.9259e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 1.00000\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 5s 494ms/step - loss: 0.1096 - acc: 0.8962 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.5636e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 1.00000\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 6s 556ms/step - loss: 0.3343 - acc: 0.9056 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.7151e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 1.00000\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 6s 640ms/step - loss: 0.1562 - acc: 0.9025 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7321e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.162277292675049e-06.\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 1.00000\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 7s 675ms/step - loss: 0.4544 - acc: 0.9025 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.2542e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 1.00000\n",
      "Epoch 18/40\n",
      "10/10 [==============================] - 9s 859ms/step - loss: 0.0487 - acc: 0.9968 - top_k_categorical_accuracy: 1.0000 - val_loss: 9.1457e-05 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 1.00000\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 8s 768ms/step - loss: 0.0261 - acc: 0.9968 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0283e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 1.00000\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 10s 978ms/step - loss: 0.0601 - acc: 0.9905 - top_k_categorical_accuracy: 1.0000 - val_loss: 5.0729e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 1.00000\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 10s 1s/step - loss: 0.0156 - acc: 0.9968 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.6926e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999115286567e-07.\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 1.00000\n",
      "Epoch 22/40\n",
      "10/10 [==============================] - 10s 988ms/step - loss: 0.0224 - acc: 0.9968 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.8895e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 1.00000\n",
      "Epoch 23/40\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.0227 - acc: 0.9938 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.9605e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 1.00000\n",
      "Epoch 24/40\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.0245 - acc: 0.9937 - top_k_categorical_accuracy: 1.0000 - val_loss: 8.7585e-05 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 1.00000\n",
      "Epoch 25/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.2920 - acc: 0.9056 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.4203e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 1.00000\n",
      "Epoch 26/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0551 - acc: 0.9874 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.9715e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 5e-07.\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 1.00000\n",
      "Epoch 27/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0162 - acc: 0.9968 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.8434e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 1.00000\n",
      "Epoch 28/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0210 - acc: 0.9937 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.9756e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 1.00000\n",
      "Epoch 29/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0282 - acc: 0.9937 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.0934e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 1.00000\n",
      "Epoch 30/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0242 - acc: 0.9937 - top_k_categorical_accuracy: 1.0000 - val_loss: 8.1572e-05 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 1.00000\n",
      "Epoch 31/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0314 - acc: 0.9905 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.2502e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 1.00000\n",
      "Epoch 32/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0234 - acc: 0.9937 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.5417e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 1.00000\n",
      "Epoch 33/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0367 - acc: 0.9874 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.4584e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 1.00000\n",
      "Epoch 34/40\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.0112 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 9.9977e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 1.00000\n",
      "Epoch 35/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0211 - acc: 0.9968 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.1936e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 1.00000\n",
      "Epoch 36/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.1917 - acc: 0.9056 - top_k_categorical_accuracy: 1.0000 - val_loss: 5.0530e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 1.00000\n",
      "Epoch 37/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.3056 - acc: 0.9025 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.7431e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 1.00000\n",
      "Epoch 38/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0117 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 6.9746e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 1.00000\n",
      "Epoch 39/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0225 - acc: 0.9968 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.6515e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 1.00000\n",
      "Epoch 40/40\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.1231 - acc: 0.9056 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.7276e-04 - val_acc: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc','top_k_categorical_accuracy'])\n",
    "\n",
    "#save_best = ModelCheckpoint(filepath='D:/polynomial/covid19/inception_dl4_val_new/checkpoint-{val_acc:.4f}.h5',monitor='val_acc',mode='auto',save_best_only=True)\n",
    "save_best=ModelCheckpoint('D:/polynomial/covid19/anomaly dataset/RAM_Net.h5', verbose=1,monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=np.sqrt(0.1),\n",
    "                                            min_lr=0.5e-6, cooldown=0,mode='auto')\n",
    "callbacks=[learning_rate_reduction,tensorboard,save_best]\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "history = model.fit_generator(train_datagen.flow(X_train,y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = val_datagen.flow(X_val, y_val),\n",
    "                              verbose = 1, steps_per_epoch=(X_train.shape[0] // batch_size),\n",
    "                              validation_steps=(X_val.shape[0] // batch_size),\n",
    "                              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation\n",
    "\n",
    "For the performance evaluation, I use several performance evaluation matrices:\n",
    "1. Test accuracy\n",
    "2. confusion matrix\n",
    "3. Precision\n",
    "4. Recall\n",
    "5. F1 Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 3ms/step\n",
      "Validation accuracy= 1.000000  ;  loss_v= 0.031549\n",
      "True Label for the test set:  [1 0 1 1 1 1 0]\n",
      "Predicted label for the test set: [1 0 1 1 1 1 0]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Non_Problematic       1.00      1.00      1.00         2\n",
      "    Problematic       1.00      1.00      1.00         5\n",
      "\n",
      "       accuracy                           1.00         7\n",
      "      macro avg       1.00      1.00      1.00         7\n",
      "   weighted avg       1.00      1.00      1.00         7\n",
      "\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "Test: accuracy = 1.000000  ;  loss = 0.088632\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "confusion matrix [[2 0]\n",
      " [0 5]]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('D:/polynomial/covid19/anomaly dataset/RAM_Net.h5')\n",
    "loss_val, acc_val,top5 = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(\"Validation accuracy= %f  ;  loss_v= %f\" % (acc_val, loss_val))\n",
    "#print('top5 validate accuracy: ',top5)\n",
    "#print('top3 validate accuracy: ',top3)\n",
    "\n",
    "\n",
    "X_test = np.load(\"D:/polynomial/covid19/anomaly dataset/test.npy\")\n",
    "y_test = np.load(\"D:/polynomial/covid19/anomaly dataset/test_labels.npy\")\n",
    "#print(\"1st ytest: \",y_test)\n",
    "print(\"True Label for the test set: \",y_test)\n",
    "target_names = ['Non_Problematic', 'Problematic']\n",
    "#print(\"1st y_test\",y_test, y_test.shape)\n",
    "y_pred = model.predict(X_test)\n",
    "#print(\"Test Accuracy\",y_pred.astype(float),y_pred.shape,type(y_pred))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "#print(\"1: \",y_pred)\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "#print(\"2: \",y_pred)\n",
    "\n",
    "y_classes = y_pred.argmax(axis=-1)\n",
    "print(\"Predicted label for the test set:\",y_classes)\n",
    "\n",
    "#print(\"5th y_pred\",y_classes,y_classes.shape)\n",
    "print(classification_report(y_test, y_classes, target_names=target_names))\n",
    "y_test = to_categorical(y_test)\n",
    "#print(\"2nd y_test\",y_test)\n",
    "\n",
    "#print(\"2nd ytest: \",y_test)\n",
    "\n",
    "loss_test, acc_test,top5 = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (acc_test, loss_test))\n",
    "#print(\"top5 test accuracy: \",top5)\n",
    "#print(\"top3 test accuracy: \",top3)\n",
    "\n",
    "\n",
    "pre=model.predict(X_test,verbose=1)\n",
    "#print(\"pre\",pre)\n",
    "n_values=3\n",
    "c = np.eye(n_values, dtype=int)[np.argmax(pre, axis=1)]\n",
    "#print('one-hot-encoding',c)\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), c.argmax(axis=1))\n",
    "print(\"confusion matrix\",cm)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#print('new_cm',cm)\n",
    "#print('acc',cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To sum up, all the evaluation functions prove that My algorithm (RAM-Net) perfectly detects the problematic and non_problematic images with 100% test accuracy.\n",
    "\n",
    "In the Segmentation_model_extend_ai.ipynb I provide the segmentation related solution for this same problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
